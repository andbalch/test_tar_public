{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('test_tar_json_parse': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8a460167d8aa885cb47d5964c71ac3986484241d23a3d8e5e10c68c298a08e19"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Exploration JSON Parser\n",
    "The goal of this notebook is to flatten and extract the data into .csv from .apk.json files included un the MalDroid dataset containing the CopperDroid analysis of android APKs. This notebook focuses on the data included under the dynamic:host: headers of the JSON file. This is done to be able to more easily visualize and analyze the houndreds of thousands of rows of sys calls. \n",
    "## Objectives\n",
    "1. Isolate the data under the 'dynamic' header into a JSON array under the header 'host'\n",
    "2. Create seperate JSON files of objects with the same \"class\" attribute\n",
    "3. Record a nested dictionary of common attributes associated with each \"class\" attribute\n",
    "4. Flatten \"class\"-separated JSON files into their own CSVs for analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from flatten_json import flatten\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open('075049984D2937039DDE452818BC6B844C8C8CD17232DB8D951306F02234B2EA/sample_for_analysis.apk.json') as path:\n",
    "    full_json = json.load(path)"
   ]
  },
  {
   "source": [
    "# Objective 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isolated_json = full_json['behaviors']['dynamic']"
   ]
  },
  {
   "source": [
    "# Objective 2\n",
    "A list of all class names is compilied and the dictionary is formed. Then the dictionary entries are exported to JSON and a dictionary is formed and exported of the relative file paths for each class for refrence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_list = []\n",
    "class_dict = {}\n",
    "\n",
    "for item in isolated_json['host']:\n",
    "    if type(item) != dict:\n",
    "        print(\"item not of type dict\")\n",
    "        break\n",
    "    item_class = item['class']\n",
    "    if item_class not in class_list:\n",
    "        class_list.append(item_class)\n",
    "        class_dict[item_class] = [item]\n",
    "    else:\n",
    "        class_dict[item_class].append(item)\n",
    "\n",
    "#parsing strings of subdicts, see Objective 4 for explanation\n",
    "def reformat(dict_in):\n",
    "    updated_dict = {}\n",
    "    for key in dict_in.keys():\n",
    "        attribute = dict_in[key]\n",
    "        if key == 'blob' and type(attribute) is str:\n",
    "            if '{' in attribute:\n",
    "                attribute = attribute.replace(\"L,\", \",\")\n",
    "                attribute = attribute.replace(\"L}\", \"}\")\n",
    "                attribute = attribute.replace(\"u\\'\", \"\\'\")\n",
    "                try:\n",
    "                    attribute = ast.literal_eval(attribute)\n",
    "                except:\n",
    "                    print(dict_in)\n",
    "        if type(attribute) is dict:\n",
    "            updated_dict[key] = reformat(attribute)\n",
    "        elif type(attribute) is list:\n",
    "        #WARNING: This does not account for n-dimensional lists\n",
    "            updated_list = []\n",
    "            for entry in attribute:\n",
    "                if type(entry) is dict:\n",
    "                    updated_list.append(reformat(entry))\n",
    "                else:\n",
    "                    updated_list.append(entry)\n",
    "            updated_dict[key] = updated_list\n",
    "        else:\n",
    "            updated_dict[key] = attribute\n",
    "    return updated_dict\n",
    "\n",
    "for key_value in class_dict.keys():\n",
    "    updated_values = []\n",
    "    for entry in class_dict[key_value]:\n",
    "        updated_entry = reformat(entry)\n",
    "        updated_values.append(updated_entry)\n",
    "    class_dict[key_value] = updated_values\n",
    "\n",
    "file_path_dict = {}\n",
    "\n",
    "for class_type in class_dict.keys():\n",
    "    holder_dict = {'content': class_dict[class_type]}\n",
    "    path = 'data_exploration/json_files/' + class_type + '.json'\n",
    "    file_path_dict[class_type] = path\n",
    "    with open('data_exploration/json_files/' + class_type + '.json', 'w') as write_json:\n",
    "        json.dump(holder_dict, write_json)\n",
    "\n",
    "with open('data_exploration/relative_class_file_paths.json', 'w') as write_json:\n",
    "        json.dump(file_path_dict, write_json)"
   ]
  },
  {
   "source": [
    "# Objective 3\n",
    "Variances in the structure of objects even within the same class need to be noted and accounted for. Each dictionary contains a list of the possible structures within a class. Attributes are treated as keys in the dictionary, where the values are their corresponding dtypes or sub-Attributes with their own dictionaries.\n",
    "The end result is a JSON object where the primary Attributes are each class type, containing a list of dictionaries of each possible structure within that class.\n",
    "## Issues parsing lists\n",
    "- Encountered issues documenting the contents of lists where where arguments and parameters keys in the binder and syscalls classes contain a list of varied string parameters and JSON\n",
    "- These are of varying sizes and variables, but will not be included in the attribute documentation for brevity and the lack of a reliable/repeatable documentation process\n",
    "- These features could be valuble, so they will need to be dealt with/taken into consideration during feature engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "class_attributes_dict = {}\n",
    "\n",
    "def dictparse(item):\n",
    "    #A bit messy, but seems to be accurate. Had to workaround some strange formatting, long dtypes, and unicode\n",
    "    attribute_dict = {}\n",
    "    for key in item.keys():\n",
    "        attribute = item[key]\n",
    "        # if key == 'blob' and type(attribute) is str:\n",
    "        #     if '{' in attribute:\n",
    "        #         attribute = attribute.replace(\"L,\", \",\")\n",
    "        #         attribute = attribute.replace(\"L}\", \"}\")\n",
    "        #         attribute = attribute.replace(\"u\\'\", \"\\'\")\n",
    "        #         try:\n",
    "        #             attribute = ast.literal_eval(attribute)\n",
    "        #         except:\n",
    "        #             print(item)\n",
    "        if type(attribute) is dict:\n",
    "            attribute_dict[key] = dictparse(attribute)\n",
    "        elif type(attribute) is list:\n",
    "            #WARNING: This does not account for n-dimensional lists\n",
    "            attribute_list = []\n",
    "            for entry in attribute:\n",
    "                if type(entry) is dict:\n",
    "                    to_validate = dictparse(entry)\n",
    "                    if to_validate not in attribute_list:\n",
    "                        attribute_list.append(to_validate)\n",
    "                # else:\n",
    "                #     attribute_list.append(str(type(attribute)))\n",
    "            attribute_dict[key] = attribute_list\n",
    "        else:\n",
    "            attribute_dict[key] = str(type(attribute))\n",
    "    return attribute_dict\n",
    "\n",
    "for class_type in class_dict.keys():\n",
    "    class_attributes_dict[class_type] = []\n",
    "    for item in class_dict[class_type]:\n",
    "        item_dict = dictparse(item)\n",
    "        if item_dict not in class_attributes_dict[class_type]:\n",
    "            class_attributes_dict[class_type].append(item_dict)\n",
    "\n",
    "with open('data_exploration/class_attributes.json', 'w') as write_json:\n",
    "        json.dump(class_attributes_dict, write_json)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "# Objective 4\n",
    "In order to correctly flatten the json data, subdictionaries in entries that are enclosed by double quotes need to be properly formatted into dictionaries/JSON. This fucntionality will be added to Objective 2 and removed from Objective 3. It is also worth keeping in mind that each entry in the list under the content header will need to be flattened seperately and incrementally added to the dataframe, otherwise every entry will be along the x-axis. Additionally, the different data formats for different classes as displayed in Objective 3 may introduce none/NaN values. These will need to be imputed a some point and may cause issues exporting the dataframes to csv or ingesting them into data visalization software/libraries. Thankfully, imputation will be made easier by the steps taken to document each "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample flattening of a test json into csv\n",
    "# path = file_path_dict['FS PIPE ACCESS']\n",
    "\n",
    "# with open(path) as read:\n",
    "#     json_to_flat = json.load(read)\n",
    "\n",
    "# list1 = json_to_flat['content']\n",
    "# flat = flatten(list1[0])\n",
    "# flat = json_normalize(flat)\n",
    "# flat = pd.DataFrame(flat)\n",
    "\n",
    "# flat1 = flatten(list1[2])\n",
    "# flat1 = json_normalize(flat1)\n",
    "# flat1 = pd.DataFrame(flat1)\n",
    "\n",
    "# pd.concat([flat, flat1], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_export (structured_json, key):\n",
    "    content_list = structured_json['content']\n",
    "    first_element = flatten(content_list[0])\n",
    "    first_element = json_normalize(first_element)\n",
    "    dataframe1 = pd.DataFrame(first_element)\n",
    "    del content_list[0]\n",
    "    for data in content_list:\n",
    "        data = flatten(data)\n",
    "        data = json_normalize(data)\n",
    "        dataframe2 = pd.DataFrame(data)\n",
    "        dataframe1 = pd.concat([dataframe1, dataframe2], axis=0, ignore_index=True)\n",
    "    with open('data_exploration/csv_files/' + key + '.csv', 'w') as write_csv:\n",
    "        dataframe1.to_csv(write_csv)\n",
    "\n",
    "for key, value in file_path_dict.items():\n",
    "    with open(value, 'r') as path:\n",
    "        flatten_and_export(json.load(path), key)"
   ]
  }
 ]
}